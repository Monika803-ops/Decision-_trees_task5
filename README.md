

## **Task 5 â€“ Decision Trees, Ensemble Learning & Feature Importance**

### **1. Objective**

In this task, I aimed to:

* Train a **Decision Tree** model for heart disease prediction.
* Limit the tree depth to compare performance.
* Use **Random Forest** for better accuracy and generalization.
* Calculate **feature importance** to understand which features impact predictions the most.
* Visualize decision trees using **Graphviz**.

---

### **2. Dataset**

We used the **Heart Disease dataset** from Kaggle:
ðŸ”— [Heart Disease UCI Dataset](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset)

* **Input Features:** Age, cholesterol, resting blood pressure, maximum heart rate, chest pain type, etc.
* **Target Variable:**

  * `0` â†’ No Heart Disease
  * `1` â†’ Heart Disease

The dataset file **`heart.csv`** was placed in the **project root folder**.

---

### **3. Environment Setup & Library Installation**

Before running the code, we installed the required libraries.

#### **Step 1 â€“ Install scikit-learn, pandas, matplotlib**

```bash
pip install scikit-learn pandas matplotlib
```

#### **Step 2 â€“ Install Graphviz (for Decision Tree Visualization)**

**Windows:**

```bash
winget install --id Graphviz.Graphviz
```

**Mac:**

```bash
brew install graphviz
```

**Linux:**

```bash
sudo apt-get install graphviz
```

#### **Step 3 â€“ Install Python Graphviz Package**

```bash
pip install graphviz
```

---

### **4. Steps Performed**

#### **Step 1 â€“ Load and Explore Data**

* Loaded `heart.csv` using **pandas**.
* Checked for missing values and data distribution.
* Split data into **features (X)** and **target (y)**.

#### **Step 2 â€“ Train-Test Split**

* Split data into **80% training** and **20% testing** using `train_test_split`.

#### **Step 3 â€“ Decision Tree Model**

* Trained a **full Decision Tree** classifier.
* Measured accuracy on the test set.

#### **Step 4 â€“ Limited Depth Decision Tree**

* Trained another Decision Tree with `max_depth=3` to avoid overfitting.
* Compared accuracy with the full tree.

#### **Step 5 â€“ Random Forest Model**

* Trained a **Random Forest** classifier (ensemble learning of many decision trees).
* Measured accuracy and compared with the Decision Tree.

#### **Step 6 â€“ Feature Importance**

* Extracted the top features influencing predictions.
* Displayed them in a sorted table.

#### **Step 7 â€“ Cross-Validation**

* Used **5-fold cross-validation** to evaluate model stability.

#### **Step 8 â€“ Graph Visualization**

* Used `export_graphviz` from sklearn to save the tree.
* Used **Graphviz** to render the decision tree as an image.

---

### **5. Output Results**

```
Decision Tree Accuracy: 0.9853
Limited Depth Decision Tree Accuracy: 0.8000
Random Forest Accuracy: 0.9853

Feature Importances:
cp         0.135
ca         0.127
thalach    0.122
oldpeak    0.121
thal       0.110
age        0.077
chol       0.074
trestbps   0.071
exang      0.057
slope      0.045
sex        0.028
restecg    0.018
fbs        0.008

Cross-validation scores: [1.         1.         1.         1.         0.9853]
Average CV Score: 0.9970
```

---

### **6. Visualization Output**

Below is the **Random Forest output graph**:

![Random Forest](images/random_forest.png)

---

### **7. Conclusion**

* **Random Forest** provided the same high accuracy as a full Decision Tree but is generally more robust against overfitting.
* Feature importance analysis revealed that **chest pain type (cp)**, **number of major vessels (ca)**, and **maximum heart rate achieved (thalach)** were the most influential features.
* Decision Tree visualization (Graphviz) helps in understanding model decisions step-by-step.

---


